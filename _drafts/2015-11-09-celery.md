---
layout: post
title: 小试Celery
---

### {{ page.title }}

+ Celery项目结构如下:

    ~~~
    proj/__init__.py
        /celery_app.py    # 该文件是简历Celery对象
        /celeryconfig.py  # 该文件是配置celery的配置信息
        /tasks.py         # 任务信息
    ~~~

+ 配置celeryconfig.py对象
    
    **celery_app.py**如下:

    ~~~python
    from celery import Celery
    from proj import celeryconfig
    app = Celery("tasks")
    app.config_from_object(celeryconfig)
    ~~~

    **celeryconfig.py**内容如下:

    ~~~python
    # -*- coding: utf-8 -*-

    # Broker url
    BROKER_URL = "redis://localhost"
    # Backend url to store task state and results
    CELERY_RESULT_BACKEND = "redis://localhost/1"

    # List of modules to import when celery starts
    CELERY_IMPORTS = ("proj.tasks",)

    # Celery annotations
    CELERY_ANNOTATIONS = {"tasks.add": {"rate_limit": "10/s"}}

    # The number of concurrent worker processes/threads/green threads executing
    # tasks.
    # If you’re doing mostly I/O you can have more processes, but if mostly
    # CPU-bound, try to keep it close to the number of CPUs on your machine.
    # If not set, the number of CPUs/cores on the host will be used.
    # Defaults to the number of available CPUs.
    CELERYD_CONCURRENCY = 4

    # How many messages to prefetch at a time multiplied by the number of
    # concurrent processes. The default is 4 (four messages for each process).
    # The default setting is usually a good choice, however – if you have very
    # long running tasks waiting in the queue and you have to start the workers,
    #  note that the first worker to start will receive four times the number of
    # messages initially. Thus the tasks may not be fairly distributed to the
    # workers.
    # To disable prefetching, set CELERYD_PREFETCH_MULTIPLIER to 1.
    # Setting CELERYD_PREFETCH_MULTIPLIER to 0 will allow the worker to
    # keep consuming as many messages as it wants.
    CELERYD_PREFETCH_MULTIPLIER = 1

    #
    CELERY_TASK_SERIALIZER = 'json'
    # Content-type(MIME)
    CELERY_ACCEPT_CONTENT = ['json']  # Ignore other content
    CELERY_RESULT_SERIALIZER = 'json'
    CELERY_TIMEZONE = 'Asia/Shanghai'
    CELERY_ENABLE_UTC = True
    ~~~

+ 添加tasks信息

    **tasks.py**的内容如下:

    ~~~python
    # -*- coding: utf-8 -*-
    from proj.celery_app import app


    @app.task
    def add(x, y):
        return x + y


    @app.task
    def mul(x, y):
        return x * y
    ~~~

+ 启动celery
    
    + 本地直接启动celery
        celery -A proj.celery_app worker -l info

    + 正式环境使用supervisord来管理celery进程

        ~~~python
        [program:celery_app]
        command=/opt/seems/venvs/celery -A proj.celery_app worker -l info
        directory=/home/seems/py_code/proj
        # Supervisor will start as many instances of this program as named by numprocs
        numprocs=1
        # log file
        stdout_logfile=/var/log/celery/celery.log
        # std error log file
        stderr_logfile=/var/log/celery/celery.log
        autostart=true
        autorestart=true
        killasgroup=true 
        ~~~ 

+ http Callback tasks

    如果需要使用http模块，需要设置CELERY_IMPORTS并添加celery.task.http到其中.
    当你使用GET/POST数据来调用tasks,然后返回JSON格式的结果．

    ~~~python
    GET http://example.com/mytask/?arg1=a&arg2=b&arg3=c
    # POST data needs to be form encoded
    POST http://example.com/mytask
    ~~~

